{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIMADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "\n",
    "from climada.hazard import TropCyclone\n",
    "from climada.entity import Exposures\n",
    "from climada.entity.exposures import LitPop\n",
    "from climada.entity.impact_funcs import ImpactFuncSet, ImpfTropCyclone\n",
    "\n",
    "import adjusted_year_loss_tables as aylt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set parameters\n",
    "# Exposure\n",
    "my_country = 'aus'\n",
    "m, n = 1,1 # based on analysis by Eberenz et al. (2020), (1,1) is the best combination.\n",
    "my_fin_mode = 'pc'\n",
    "ref_year = 2023\n",
    "\n",
    "# Hazard\n",
    "start_year = 1980\n",
    "end_year = 2023\n",
    "synthetic_start_year = end_year + 1\n",
    "n_synth_tracks = 1_000\n",
    "\n",
    "my_res_arcsec = 600\n",
    "starting_phase = 'Nino'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure\n",
    "exp1 = LitPop.from_countries(my_country, my_res_arcsec, (m,n), my_fin_mode, reference_year = ref_year)\n",
    "exp1.check()\n",
    "\n",
    "# Remove Lord Howe Island and Macquarie Island\n",
    "exp = Exposures(exp1.gdf[exp1.geometry.x <= 155])\n",
    "exp.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_with_state = aylt.assign_state_to_exposure(exp.data, \"STE_2021_AUST_SHP_GDA2020/STE_2021_AUST_GDA2020.shp\").replace(\"Australian Capital Territory\", \"New South Wales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = exp_with_state[\"STE_NAME21\"]\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_exposures = exp_with_state.groupby(\"STE_NAME21\")[\"value\"].sum()\n",
    "region_thresholds = dict(state_exposures * 1e-6)\n",
    "region_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in region_thresholds:\n",
    "    print(f\"{region} threshold: ${region_thresholds[region]:.4} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(f'Total exposure value to {my_country}: USD {exp.gdf.value.sum():,.2f} / AUD {exp.gdf.value.sum()/0.6630:,.2f} ({ref_year})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hazard\n",
    "haz = TropCyclone.from_hdf5(f'Hazards/haz_aus_{n_synth_tracks}synth.hdf5')\n",
    "haz.check()\n",
    "exp.assign_centroids(haz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impf_tc = ImpfTropCyclone.from_emanuel_usa()\n",
    "impf_set = ImpactFuncSet([impf_tc])\n",
    "impf_set.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_catalogues = aylt.compute_loss_catalogue(haz, exp, impf_set, save_catalogue=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_losses_timed(kwargs):\n",
    "    start_time = time.time()\n",
    "    base_filename = aylt.generate_year_loss_tables(**kwargs)\n",
    "    print(f\"Finished {base_filename} with in {time.time() - start_time:.2f} seconds\")\n",
    "    return base_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_configs = [\n",
    "    dict(homogeneous=True, loc=False, intensity=False, damage=False),\n",
    "    dict(homogeneous=False, loc=False, intensity=False, damage=False),\n",
    "    dict(homogeneous=False, loc=True, intensity=True, damage=True, p_loc=0.5),\n",
    "    dict(homogeneous=False, loc=True, intensity=True, damage=True, p_loc=0.8),\n",
    "    dict(homogeneous=False, loc=True, intensity=True, damage=True, p_loc=1.0),\n",
    "    dict(homogeneous=False, loc=False, intensity=True, damage=False),\n",
    "    dict(homogeneous=False, loc=False, intensity=False, damage=True),\n",
    "    dict(homogeneous=False, loc=True, intensity=False, damage=False, p_loc=0.5),\n",
    "    dict(homogeneous=False, loc=True, intensity=True, damage=False, p_loc=0.5),\n",
    "    dict(homogeneous=False, loc=True, intensity=False, damage=True, p_loc=0.5),\n",
    "    dict(homogeneous=False, loc=False, intensity=True, damage=True),\n",
    "]\n",
    "\n",
    "# Make a random shared seed to use for each simulation\n",
    "rng = np.random.default_rng(2025)\n",
    "seed = rng.integers(0, 2**32, dtype=np.uint32)\n",
    "\n",
    "for i, cfg in enumerate(loss_configs):\n",
    "    cfg[\"haz\"] = haz\n",
    "    cfg[\"n_sim\"] = n_sim\n",
    "    cfg[\"n_years\"] = 1\n",
    "    cfg[\"loss_catalogues\"] = loss_catalogues\n",
    "    cfg[\"starting_enso\"] = starting_phase\n",
    "    cfg[\"seed\"] = seed\n",
    "    cfg[\"regions\"] = regions\n",
    "    cfg[\"region_thresholds\"] = region_thresholds\n",
    "    cfg[\"synthetic_start_year\"] = synthetic_start_year\n",
    "    cfg[\"compound_factor\"] = 1.5\n",
    "    cfg[\"show_progress\"] = True\n",
    "    cfg[\"save_poisson_debug\"] = True\n",
    "    cfg[\"save_damage_debug\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results1 = Parallel(n_jobs=10)(\n",
    "    delayed(resample_losses_timed)(cfg)\n",
    "    for cfg in tqdm(loss_configs, desc=\"Running simulations\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Make a copy of the loss_configs list but change the n_year to 5 and make a new seed\n",
    "rng = np.random.default_rng(2026)\n",
    "seed = rng.integers(0, 2**32, dtype=np.uint32)\n",
    "\n",
    "loss_configs_5 = [cfg.copy() for cfg in loss_configs]\n",
    "for i, cfg in enumerate(loss_configs_5):\n",
    "    cfg[\"n_years\"] = 5\n",
    "    cfg[\"seed\"] = seed\n",
    "    cfg[\"show_progress\"] = True\n",
    "\n",
    "results5 = Parallel(n_jobs=10)(\n",
    "    delayed(resample_losses_timed)(cfg)\n",
    "    for cfg in tqdm(loss_configs_5, desc=\"Running simulations\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
